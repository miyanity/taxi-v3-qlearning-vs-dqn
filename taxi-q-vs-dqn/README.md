<h1 align="center">â‹† Ëšï½¡â‹†à­¨ taxi-v3 rl comparison à­§â‹† Ëšï½¡â‹†</h1>


<p align="center"><i>â‹† Ëšï½¡âŠ¹Ëšâ‚Š comparing classical rl and deep rl in a toy environment â‚ŠËšâŠ¹Ëšï½¡â‹†</i></p>

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª overview  

**taxi-v3 rl comparison** explores how **tabular q-learning** compares to **deep q-learning (dqn)** in a small, discrete environment from **ai gymnasium**.

the project focuses on **learning speed** and **training stability**, demonstrating how algorithm choice depends on environment complexity.

```bash
ðŸš• environment â†’ taxi-v3 (gymnasium toy text)
ðŸ§  methods â†’ tabular q-learning Â· deep q-learning (dqn)
ðŸ“ˆ comparison â†’ learning speed Â· stability
ðŸŽ¯ goal â†’ understand when deep rl is necessary vs overkill
```

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª tech stack  

**languages** python  
**libraries** gymnasium Â· numpy Â· torch Â· matplotlib  
**tools** vs code Â· git Â· github  

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª methods  

- tabular q-learning â†’ q-table with epsilon-greedy exploration  
- deep q-learning (dqn) â†’ mlp-based network with replay buffer and target network  
- evaluation â†’ reward curves with moving average smoothing  

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª results  

âš¡ dqn learns faster during early training  
ðŸ“‰ q-learning is more stable after convergence  
ðŸ§  small discrete environments favor tabular methods  

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª visual preview  

image file: results/learning_curves.png  

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª how to run  

pip install -r requirements.txt  
python train_qlearning.py  
python train_dqn.py  
python plots.py  

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª key takeaway  

simpler reinforcement learning methods can outperform deep rl in structured, low-complexity environments.

---

### â‹†à±¨à§ŽËšâŸ¡Ë– à£ª about
miyah dones  
computer science + molecular biology @ towson university